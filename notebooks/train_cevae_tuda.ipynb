{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# CE-VAE + TUDA Feature-Level Alignment Training\n",
        "\n",
        "This notebook fine-tunes the CE-VAE model with TUDA-style feature-level adversarial alignment.\n",
        "\n",
        "**What this does:** Adds a feature-level discriminator that teaches the encoder to produce domain-invariant features, improving generalization on diverse real-world underwater images.\n",
        "\n",
        "**Requirements:**\n",
        "- Kaggle (T4 GPU, ~30h/week free) or Google Colab (T4 GPU)\n",
        "- ~2 hours training time\n",
        "- LSUI dataset + UIEB real underwater images\n",
        "\n",
        "**Steps:**\n",
        "1. Setup environment\n",
        "2. Upload/download datasets\n",
        "3. Train the TUDA-enhanced model\n",
        "4. Evaluate and compare with baseline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Environment Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check GPU availability\n",
        "!nvidia-smi\n",
        "import torch\n",
        "print(f\"\\nPyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"VRAM: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Clone the repository\n",
        "!git clone https://github.com/priyanshuharshbodhi1/ce-vae-tuda-underwater-enhancement.git\n",
        "%cd ce-vae-tuda-underwater-enhancement\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install dependencies\n",
        "!pip install -q albumentations lightning omegaconf opencv-python-headless \\\n",
        "    Pillow pytorch-msssim torchmetrics wandb scikit-image termcolor lpips"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Dataset Setup\n",
        "\n",
        "You need:\n",
        "1. **LSUI dataset** (paired: degraded + ground truth) \u2014 for standard CE-VAE training\n",
        "2. **UIEB real images** (unpaired) \u2014 for TUDA feature alignment\n",
        "3. **Pre-trained CE-VAE checkpoint** \u2014 as starting point"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 2. Dataset Setup (ROBUST AUTO-LINKING)\n",
        "import os\n",
        "import glob\n",
        "from pathlib import Path\n",
        "\n",
        "os.makedirs('data', exist_ok=True)\n",
        "\n",
        "def link_path(src, dst):\n",
        "    if os.path.exists(dst):\n",
        "        if os.path.islink(dst): os.unlink(dst)\n",
        "        else: return # skip folders\n",
        "    os.symlink(src, dst)\n",
        "    print(f'\u2705 Linked {src} -> {dst}')\n",
        "\n",
        "print('Searching for datasets in /kaggle/input (this may take a few seconds)...')\n",
        "\n",
        "# --- 1. Robust Search for LSUI ---\n",
        "lsui_dirs = glob.glob('/kaggle/input/**/LSUI', recursive=True)\n",
        "if lsui_dirs:\n",
        "    lsui_root = lsui_dirs[0]\n",
        "    link_path(os.path.join(lsui_root, 'input'), 'data/input')\n",
        "    link_path(os.path.join(lsui_root, 'GT'), 'data/GT')\n",
        "else:\n",
        "    print('\u26a0\ufe0f LSUI folder not found in /kaggle/input (searching for internal folders...)')\n",
        "    input_search = glob.glob('/kaggle/input/**/input', recursive=True)\n",
        "    gt_search = glob.glob('/kaggle/input/**/GT', recursive=True)\n",
        "    if input_search and gt_search:\n",
        "        link_path(input_search[0], 'data/input')\n",
        "        link_path(gt_search[0], 'data/GT')\n",
        "\n",
        "# --- 2. Robust Search for UIEB ---\n",
        "uieb_search = glob.glob('/kaggle/input/**/raw-890', recursive=True)\n",
        "if uieb_search:\n",
        "    link_path(uieb_search[0], 'data/real_underwater')\n",
        "else:\n",
        "    print('\u26a0\ufe0f UIEB (raw-890) not found. Checking root titles...')\n",
        "    root_scan = glob.glob('/kaggle/input/**/UIEB*', recursive=True)\n",
        "    if root_scan: link_path(root_scan[0], 'data/real_underwater')\n",
        "\n",
        "# --- 3. Robust Search for Checkpoint ---\n",
        "ckpt_search = glob.glob('/kaggle/input/**/*.ckpt', recursive=True)\n",
        "if ckpt_search:\n",
        "    link_path(ckpt_search[0], 'data/lsui-cevae-epoch119.ckpt')\n",
        "else:\n",
        "    print('\u26a0\ufe0f Checkpoint (.ckpt) not found in /kaggle/input')\n",
        "\n",
        "print('\\n--- Automated Dataset Indexing ---')\n",
        "def create_list_file(img_dir, out_file, label):\n",
        "    if os.path.exists(img_dir):\n",
        "        paths = sorted(glob.glob(os.path.join(img_dir, '**', '*.*'), recursive=True))\n",
        "        paths = [p for p in paths if p.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp'))]\n",
        "        with open(out_file, 'w') as f:\n",
        "            f.write('\\n'.join(paths))\n",
        "        print(f'\u2705 Generated {out_file} ({len(paths)} {label} images)')\n",
        "    else: print(f'\u274c Cannot index {label}: {img_dir} missing')\n",
        "\n",
        "# Generate train/val splits for LSUI (80/20 split of whatever is found)\n",
        "if os.path.exists('data/input') and os.path.exists('data/GT'):\n",
        "    inputs = sorted(glob.glob('data/input/*.*'))\n",
        "    targets = sorted(glob.glob('data/GT/*.*'))\n",
        "    if len(inputs) == len(targets) and len(inputs) > 0:\n",
        "        split_idx = int(0.9 * len(inputs)) # Using 90/10 for more training data\n",
        "        # Train\n",
        "        with open('data/LSUI_train_input.txt', 'w') as f: f.write('\\n'.join(inputs[:split_idx]))\n",
        "        with open('data/LSUI_train_target.txt', 'w') as f: f.write('\\n'.join(targets[:split_idx]))\n",
        "        # Val\n",
        "        with open('data/LSUI_val_input.txt', 'w') as f: f.write('\\n'.join(inputs[split_idx:]))\n",
        "        with open('data/LSUI_val_target.txt', 'w') as f: f.write('\\n'.join(targets[split_idx:]))\n",
        "        print(f'\u2705 LSUI Indexed: {split_idx} train, {len(inputs)-split_idx} val samples')\n",
        "\n",
        "create_list_file('data/real_underwater', 'data/real_underwater_images.txt', 'Real Unpaired')\n",
        "\n",
        "print('\\nFinal Data Check:')\n",
        "!ls -la data/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Verify everything is in place\n",
        "required_files = [\n",
        "    'data/lsui-cevae-epoch119.ckpt',\n",
        "    'data/LSUI_train_input.txt',\n",
        "    'data/LSUI_train_target.txt', \n",
        "    'data/LSUI_val_input.txt',\n",
        "    'data/LSUI_val_target.txt',\n",
        "    'data/real_underwater_images.txt',\n",
        "]\n",
        "\n",
        "all_good = True\n",
        "for f in required_files:\n",
        "    exists = os.path.exists(f)\n",
        "    status = '\u2705' if exists else '\u274c'\n",
        "    print(f\"{status} {f}\")\n",
        "    if not exists:\n",
        "        all_good = False\n",
        "\n",
        "if all_good:\n",
        "    print(\"\\n\ud83c\udf89 All files ready! Proceed to training.\")\n",
        "else:\n",
        "    print(\"\\n\u26a0\ufe0f Some files are missing. Please upload them before training.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Training\n",
        "\n",
        "Fine-tune CE-VAE with TUDA feature alignment. Expected time: **~1.5-2 hours on T4**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Optional: Login to Weights & Biases for experiment tracking\n",
        "# If you don't have a W&B account, you can skip this (training still works)\n",
        "import wandb\n",
        "try:\n",
        "    wandb.login()\n",
        "except:\n",
        "    print(\"W&B login skipped. Training will still run but without online logging.\")\n",
        "    os.environ['WANDB_MODE'] = 'offline'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Start training!\n",
        "!python main.py -cfg configs/cevae_E2E_lsui_tuda.yaml --trainer.max_epochs 30"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Find the best checkpoint\n",
        "import glob\n",
        "ckpt_files = sorted(glob.glob('training_logs/*/checkpoints/*.ckpt'))\n",
        "print(\"Available checkpoints:\")\n",
        "for c in ckpt_files:\n",
        "    size_mb = os.path.getsize(c) / (1024**2)\n",
        "    print(f\"  {c} ({size_mb:.1f} MB)\")\n",
        "\n",
        "# Use the last checkpoint for evaluation\n",
        "BEST_CKPT = [c for c in ckpt_files if 'last' in c][-1] if any('last' in c for c in ckpt_files) else ckpt_files[-1]\n",
        "print(f\"\\nUsing checkpoint: {BEST_CKPT}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Evaluation & Comparison\n",
        "\n",
        "Compare the TUDA-enhanced model against the original CE-VAE baseline."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluate both models on the same test set\n",
        "import sys\n",
        "sys.path.insert(0, '.')\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from omegaconf import OmegaConf\n",
        "from src.build.from_config import instantiate_from_config\n",
        "from src.metrics import compute as compute_metrics\n",
        "from tqdm import tqdm\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "def load_model(config_path, ckpt_path):\n",
        "    \"\"\"Load a CE-VAE model from config and checkpoint.\"\"\"\n",
        "    config = OmegaConf.load(config_path)\n",
        "    \n",
        "    # Override ckpt_path to None (we'll load manually)\n",
        "    config.model.params.ckpt_path = None\n",
        "    \n",
        "    # Handle TUDA-specific params for loading\n",
        "    if 'real_images_list_file' in config.model.params:\n",
        "        config.model.params.real_images_list_file = None\n",
        "    if 'real_images_dir' in config.model.params:\n",
        "        config.model.params.real_images_dir = None\n",
        "    \n",
        "    model = instantiate_from_config(config.model)\n",
        "    sd = torch.load(ckpt_path, map_location='cpu')['state_dict']\n",
        "    model.load_state_dict(sd, strict=False)\n",
        "    return model.eval().to(device)\n",
        "\n",
        "print(\"Loading baseline CE-VAE...\")\n",
        "model_baseline = load_model('configs/cevae_E2E_lsui.yaml', 'data/lsui-cevae-epoch119.ckpt')\n",
        "\n",
        "print(\"Loading TUDA-enhanced CE-VAE...\")\n",
        "model_tuda = load_model('configs/cevae_E2E_lsui_tuda.yaml', BEST_CKPT)\n",
        "\n",
        "print(\"Both models loaded!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run evaluation on validation set\n",
        "from torch.utils.data import DataLoader\n",
        "from src.data.image_enhancement import DatasetTestFromImageFileList\n",
        "\n",
        "# Load test dataset\n",
        "test_dataset = DatasetTestFromImageFileList(\n",
        "    size=256,\n",
        "    test_images_list_file='data/LSUI_val_input.txt',\n",
        "    test_target_images_list_file='data/LSUI_val_target.txt'\n",
        ")\n",
        "test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False, num_workers=2)\n",
        "\n",
        "def evaluate_model(model, loader, name):\n",
        "    \"\"\"Evaluate a model and return average metrics.\"\"\"\n",
        "    metrics_sum = {'psnr': 0, 'ssim': 0, 'uiqm': 0, 'uciqe': 0}\n",
        "    count = 0\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(loader, desc=f'Evaluating {name}'):\n",
        "            x = batch['image'].permute(0, 3, 1, 2).float().to(device)\n",
        "            y = batch['target'].permute(0, 3, 1, 2).float().to(device)\n",
        "            \n",
        "            xrec = model(x)\n",
        "            \n",
        "            # Convert to numpy for metrics\n",
        "            y_np = torch.clamp(y, -1, 1).detach().cpu()\n",
        "            y_np = ((y_np + 1) / 2 * 255).permute(0, 2, 3, 1).numpy().astype(np.uint8)\n",
        "            \n",
        "            xrec_np = torch.clamp(xrec, -1, 1).detach().cpu()\n",
        "            xrec_np = ((xrec_np + 1) / 2 * 255).permute(0, 2, 3, 1).numpy().astype(np.uint8)\n",
        "            \n",
        "            for rec, gt in zip(xrec_np, y_np):\n",
        "                res = compute_metrics(rec, gt)\n",
        "                for k in metrics_sum:\n",
        "                    metrics_sum[k] += res[k]\n",
        "                count += 1\n",
        "    \n",
        "    return {k: v / count for k, v in metrics_sum.items()}\n",
        "\n",
        "print(\"Evaluating baseline...\")\n",
        "baseline_metrics = evaluate_model(model_baseline, test_loader, 'Baseline CE-VAE')\n",
        "\n",
        "print(\"\\nEvaluating TUDA-enhanced...\")\n",
        "tuda_metrics = evaluate_model(model_tuda, test_loader, 'CE-VAE + TUDA')\n",
        "\n",
        "# Print comparison and save to file\n",
        "results_txt = \"\"\n",
        "header = f\"{'Metric':<10} {'Baseline':>12} {'TUDA-Enhanced':>15} {'\u0394':>10}\\n\" + \"-\"*50 + \"\\n\"\n",
        "results_txt += \"=\"*60 + \"\\nRESULTS COMPARISON\\n\" + \"=\"*60 + \"\\n\" + header\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"RESULTS COMPARISON\")\n",
        "print(\"=\"*60)\n",
        "print(header.strip())\n",
        "\n",
        "for k in ['psnr', 'ssim', 'uiqm', 'uciqe']:\n",
        "    delta = tuda_metrics[k] - baseline_metrics[k]\n",
        "    arrow = '\u2191' if delta > 0 else '\u2193'\n",
        "    line = f\"{k.upper():<10} {baseline_metrics[k]:>12.4f} {tuda_metrics[k]:>15.4f} {arrow}{abs(delta):>8.4f}\"\n",
        "    print(line)\n",
        "    results_txt += line + \"\\n\"\n",
        "results_txt += \"=\"*60 + \"\\n\"\n",
        "\n",
        "with open('results_comparison.txt', 'w') as f:\n",
        "    f.write(results_txt)\n",
        "print(f\"\\n\u2705 Results saved to results_comparison.txt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visual comparison\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Get a few samples\n",
        "sample_batch = next(iter(DataLoader(test_dataset, batch_size=4, shuffle=True)))\n",
        "x = sample_batch['image'].permute(0, 3, 1, 2).float().to(device)\n",
        "y = sample_batch['target'].permute(0, 3, 1, 2).float().to(device)\n",
        "\n",
        "with torch.no_grad():\n",
        "    rec_baseline = model_baseline(x)\n",
        "    rec_tuda = model_tuda(x)\n",
        "\n",
        "def tensor_to_img(t):\n",
        "    t = torch.clamp(t, -1, 1)\n",
        "    return ((t + 1) / 2).cpu().permute(1, 2, 0).numpy()\n",
        "\n",
        "fig, axes = plt.subplots(4, 4, figsize=(20, 20))\n",
        "for i in range(4):\n",
        "    axes[i, 0].imshow(tensor_to_img(x[i]))\n",
        "    axes[i, 0].set_title('Input' if i == 0 else '', fontsize=14)\n",
        "    axes[i, 0].axis('off')\n",
        "    \n",
        "    axes[i, 1].imshow(tensor_to_img(rec_baseline[i]))\n",
        "    axes[i, 1].set_title('Baseline CE-VAE' if i == 0 else '', fontsize=14)\n",
        "    axes[i, 1].axis('off')\n",
        "    \n",
        "    axes[i, 2].imshow(tensor_to_img(rec_tuda[i]))\n",
        "    axes[i, 2].set_title('CE-VAE + TUDA' if i == 0 else '', fontsize=14)\n",
        "    axes[i, 2].axis('off')\n",
        "    \n",
        "    axes[i, 3].imshow(tensor_to_img(y[i]))\n",
        "    axes[i, 3].set_title('Ground Truth' if i == 0 else '', fontsize=14)\n",
        "    axes[i, 3].axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('comparison_results.png', dpi=150, bbox_inches='tight')\n",
        "plt.show()\n",
        "print(\"\\nComparison saved to comparison_results.png\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Download the trained checkpoint\n",
        "# For Kaggle: it will be saved in the output\n",
        "# For Colab: download or save to Drive\n",
        "\n",
        "import shutil\n",
        "output_ckpt = 'cevae_tuda_finetuned.ckpt'\n",
        "shutil.copy(BEST_CKPT, output_ckpt)\n",
        "print(f\"\\n\u2705 Final checkpoint saved as: {output_ckpt}\")\n",
        "print(f\"   Size: {os.path.getsize(output_ckpt) / (1024**2):.1f} MB\")\n",
        "\n",
        "# For Colab: uncomment to save to Google Drive\n",
        "# shutil.copy(output_ckpt, '/content/drive/MyDrive/cevae_tuda_finetuned.ckpt')\n",
        "# print(\"Saved to Google Drive!\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}